{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장 Naive Bayes\n",
    "\n",
    "## 학습목표\n",
    "   - 분류를 위해 확률 분포 사용하기\n",
    "   - 나이브 베이스 분류기 학습하기\n",
    "   - RSS 피드에서 제공되는 데이터 구문 분석하기\n",
    "   - 지역적인 태도를 알아보기 위해 나이브 베이스 사용하기\n",
    "\n",
    "## 4.1 베이지안 의사결정 이론으로 분류하기\n",
    "### 장점\n",
    "   - 소량의 데이터를 가지고 작업이 이루어짐\n",
    "   - 여러개의 분류 항목을 다룰 수 있음\n",
    "    \n",
    "### 단점\n",
    "   - 입력 데이터를 어떻게 준비하느냐에 따라 민감하게 작용함\n",
    "    \n",
    "### 적용\n",
    "   - 명목형 값\n",
    "    \n",
    "## 4.2 조건부 확률\n",
    "   - '양동이 B에 주어진 회색 돌의 확률' => P(gray|bucketB)\n",
    "   - P(gray|bucketB)=P(gray and bucketB)/P(bucketB)\n",
    "    \n",
    "   - p(x|c)를 알고 있는 상태에서 P(c|x)는 베이즈 규칙을 이용하여 다음과 같이 구할 수 있음\n",
    "   - P(c|x)=P(x|c)p(c)/P(x)\n",
    "\n",
    "## 4.3 조건부 확률로 분류하기\n",
    "   ### 베이스 정리 이론\n",
    "- 만약 p1(x,y)>p2(x,y)이면, 분류 항목 1에 속함\n",
    "- 만약 p2(x,y)>p1(x,y)이면, 분류 항목 2에 속함\n",
    "    \n",
    "### 베이지안 분류 규칙\n",
    "- p(ci|x,y)=p(x,y|ci)p(ci)/p(x,y)\n",
    "- 만약 P(c1|x,y)>P(c2|x,y)이면, 분류 항목 c1에 속함\n",
    "- 만약 P(c1|x,y)<P(c2|x,y)이면, 분류 항목 c2에 속함\n",
    "    \n",
    "## 4.4 나이브 베이스로 문서 분류하기\n",
    "### 나이브 베이스에 대한 일반적인 접근 방법\n",
    "1. 수집: 많은 방법이 있으나 이번 장에서는 RSS 자료를 사용\n",
    "2. 준비: 명목형 또는 부울 형(Boolean)값이 요구됨\n",
    "3. 분석: 많은 속성들을 플롯하는 것은 도움이 되지 못함. 히스토그램으로 보는 것이 가장 좋음\n",
    "4. 훈련: 각 속성을 독립적으로 조건부 확률을 계산함\n",
    "5. 검사: 오류율을 계산함\n",
    "6. 사용: 어떤 분류를 설정하는 데 있어 나이브 베이스를 사용할 수 있음\n",
    "    <br>나이브 베이스의 일반적인 응용 프로그램 중 하나는 문서 분류이나 꼭 텍스트 분류만 가능한 것은 아님\n",
    "          \n",
    "## 4.5 파이썬으로 텍스트 분류하기\n",
    "1. 텍스트 목록을 숫자 벡터로 변환하는 방법과 이 벡터로부터 조건부 확률을 구하는 방법을 살펴봄\n",
    "2. 파이썬으로 나이브 베이스를 수행하기 위한 분류기를 생성\n",
    "3. 몇가지 실질적인 고려사항을 살펴봄\n",
    "\n",
    "### 4.5.1 준비: 텍스트로 단어 벡터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bayes.py\n",
    "\n",
    "# function loadDataSet: 예제 데이터 생성\n",
    "# output: postingList, classVec\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', \\\n",
    "                  'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', \\\n",
    "                  'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', \\\n",
    "                  'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how',\\\n",
    "                  'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1] #1: 폭력적인, 0: 폭력적이지 않음\n",
    "    return postingList,classVec\n",
    "\n",
    "\n",
    "# function createVocabList: 유일한 단어 목록 생성\n",
    "# input: dataSet \n",
    "# output: list(vocabSet) \n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([]) #비어있는 집합 생성\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet|set(document) #두 개의 집합 통합 생성\n",
    "    return list(vocabSet)\n",
    "\n",
    "\n",
    "# function setOfWords2Vec: 주어진 문서 내에 어휘 목록에 있는 단어가 존재하는지 아닌지를 확인\n",
    "# input: vocabList, inputSet\n",
    "# output: returnVec\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList) #vocabList와 같은 길이의 모두 0인 벡터 생성\n",
    "    for word in inputSet:\n",
    "        if word in vocabList: #vocabList내에 있다면\n",
    "            returnVec[vocabList.index(word)] = 1 \n",
    "        else: #vocabList내에 없다면\n",
    "            print (\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadDataSet example:\n",
      "\n",
      "listOPosts = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "\n",
      "listClasses = [0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# loadDataSet 확인\n",
    "print(\"loadDataSet example:\\n\")\n",
    "listOPosts,listClasses = loadDataSet()\n",
    "print(\"listOPosts = %s\" %listOPosts)\n",
    "print(\"\\nlistClasses = %s\" %listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createVocabList example:\n",
      "['mr', 'I', 'to', 'posting', 'ate', 'my', 'stupid', 'worthless', 'not', 'garbage', 'park', 'take', 'please', 'buying', 'steak', 'him', 'love', 'dalmation', 'is', 'stop', 'has', 'quit', 'cute', 'how', 'flea', 'help', 'so', 'dog', 'food', 'licks', 'problems', 'maybe']\n",
      "\n",
      "setOfWords2Vec example:\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
      "[0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# createVocabList 확인\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "print (\"createVocabList example:\")\n",
    "print (myVocabList)\n",
    "\n",
    "# setOfWord2Vect 확인\n",
    "print(\"\\nsetOfWords2Vec example:\")\n",
    "print (setOfWords2Vec(myVocabList, listOPosts[0]))\n",
    "print (setOfWords2Vec(myVocabList, listOPosts[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 훈련: 단어 벡터로 확률 계산하기\n",
    "p(ci|<b>w</b>)=p(<b>w</b>|ci)p(ci)/p(<b>w</b>)\n",
    "<br>p(ci) :단어가 얼마나 많이 발생했는가를 가지고 구할 수 있음 즉 i번째 분류 항목을 확인한 다음 전체 문서의 수로 나눔\n",
    "<br>p(<b>w</b>|ci) = p(w0,w1,w2...wN|ci)  :이 때 모든 단어들이 독립적이라고 가정하며 이를 <b>조건부 독립</b>\n",
    "이라고 함\n",
    "따라서 이에 대한 확률은 p(w0|ci)p(w1|ci)p(w2|ci)...p(wN|ci)로 계산할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# function trainNB0: 나이브 베이스 분류기 훈련\n",
    "# input: trainMatrix, trainCategory\n",
    "# output: p0Vect, p1Vect, pAbusive\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    # 각 분류 항목(trainCategory)에 대한 문서(trainMatrix)의 개수 세기\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = np.zeros(numWords); p1Num = np.zeros(numWords)\n",
    "    p0Denom=0.0; p1Denom=0.0\n",
    "\n",
    "    # 훈련을 위한 모든 문서의 개수(numTrainDocs)만큼 반복\n",
    "    for i in range(numTrainDocs):\n",
    "        # 분류 항목 개수만큼 반복\n",
    "        if trainCategory[i]==1:\n",
    "            # 해당 토큰이 문서 내에 있다면 해당 토큰에 대한 개수를 증가\n",
    "            p1Num += (trainMatrix[i])\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        # 분류 항목 개수만큼 반복\n",
    "        else:\n",
    "            p0Num += (trainMatrix[i])\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "\n",
    "    #조건부 확률을 구하기 위해 해당 토큰의 개수를 토큰 전체의 수로 나눔\n",
    "    p1Vect = p1Num/p1Denom\n",
    "    # 토큰의 개수만큼 반복\n",
    "    p0Vect = p0Num/p0Denom\n",
    "\n",
    "    # 각 분류 항목에 대한 조건부 확률을 반환함\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainMat:[[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]]\n",
      "\n",
      "trainNB0 example:\n",
      "[ 0.04166667  0.04166667  0.04166667  0.          0.04166667  0.125       0.\n",
      "  0.          0.          0.          0.          0.          0.04166667\n",
      "  0.          0.04166667  0.08333333  0.04166667  0.04166667  0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.04166667  0.04166667\n",
      "  0.04166667  0.04166667  0.04166667  0.          0.04166667  0.04166667\n",
      "  0.        ]\n",
      "[ 0.          0.          0.05263158  0.05263158  0.          0.\n",
      "  0.15789474  0.10526316  0.05263158  0.05263158  0.05263158  0.05263158\n",
      "  0.          0.05263158  0.          0.05263158  0.          0.          0.\n",
      "  0.05263158  0.          0.05263158  0.          0.          0.          0.\n",
      "  0.          0.10526316  0.05263158  0.          0.          0.05263158]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# trainNB0 확인\n",
    "\n",
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "print (\"trainMat:%s\" %trainMat)\n",
    "\n",
    "# trainNB0 example\n",
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "print (\"\\ntrainNB0 example:\")\n",
    "print (p0V)\n",
    "print (p1V)\n",
    "print (pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 검사: 실제 조건을 반영하기 위해 분류기 수정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "# trainNB0 function\n",
    "# input\n",
    "#  - trainMatrix: Training data set\n",
    "#  - trainCategory: Class labels\n",
    "# output\n",
    "#  - p0Vect: 분류 항목 0일 확률\n",
    "#  - p1Vect: 분류 항목 1일 확률\n",
    "#  - pAbusive: 폭력적일 확률\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # initialize probabilities\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)\n",
    "    p0Denom = 2.0; p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # Vector addition\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # Element-wise division\n",
    "    p1Vect = log(p1Num / p1Denom)  # change to log()\n",
    "    p0Vect = log(p0Num / p0Denom)  # change to log()\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainMat:[[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]]\n",
      "\n",
      "trainNB0 example:\n",
      "[-2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -1.87180218\n",
      " -3.25809654 -3.25809654 -3.25809654 -3.25809654 -3.25809654 -3.25809654\n",
      " -2.56494936 -3.25809654 -2.56494936 -2.15948425 -2.56494936 -2.56494936\n",
      " -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936\n",
      " -2.56494936 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -3.25809654]\n",
      "[-3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -3.04452244\n",
      " -1.65822808 -1.94591015 -2.35137526 -2.35137526 -2.35137526 -2.35137526\n",
      " -3.04452244 -2.35137526 -3.04452244 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -2.35137526 -3.04452244 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -3.04452244 -3.04452244 -1.94591015 -2.35137526 -3.04452244\n",
      " -3.04452244 -2.35137526]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# trainNB0 확인\n",
    "\n",
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "print (\"trainMat:%s\" %trainMat)\n",
    "\n",
    "# trainNB0 example\n",
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "print (\"\\ntrainNB0 example:\")\n",
    "print (p0V)\n",
    "print (p1V)\n",
    "print (pAb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec)+log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec)+log(1.0 - pClass1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))\n",
    "    testEntry = ['love','my','dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as:', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    testEntry = ['stupid','garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as: 0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 준비: 중복 단어 문서 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "vec = bagOfWords2VecMN(myVocabList, listOPosts[0])\n",
    "print (vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 예제: 스팸 이메일 분류하기\n",
    "1. 수집: 제공된 텍스트 파일\n",
    "2. 준비: 토큰 벡터로 텍스트 구문 분석\n",
    "3. 분석: 구문 분석이 정확하게 되었는지 토큰 검토\n",
    "4. 훈련: 이전에 생성했던 trainNB0() 사용\n",
    "5. 검사: classifyNB()를 사용하고 문서 집합에서 오류율을 계산하는 새로운 검사 함수를 생성한다.\n",
    "6. 사용: 완전한 프로그램을 구축하여 문서들을 분류하고 화면에 잘못 분류된 문서들을 출력한다.\n",
    "\n",
    "### 4.6.1 준비: 텍스트 토큰 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L.', 'I', 'have', 'ever', 'laid', 'eyes', 'upon.']\n",
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm.l.', 'i', 'have', 'ever', 'laid', 'eyes', 'upon.']\n"
     ]
    }
   ],
   "source": [
    "#텍스트 분할하기\n",
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "listOfTokens = mySent.split()\n",
    "print (listOfTokens)\n",
    "\n",
    "# 모두 소문자로 변환\n",
    "listOfTokens = [tok.lower() for tok in listOfTokens if len(tok) > 0]\n",
    "print (listOfTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2 검사: 나이브 베이스로 교차 검증하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "def spamTest():\n",
    "    docList=[]; classList=[]; fullText=[]\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open('data/email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('data/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = range(50); testSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat = []; trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print ('the error rate is: %s' %float(errorCount)/len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spamTest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dea3e8b50c46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspamTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spamTest' is not defined"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 예제: 나이브 베이스를 사용하여 개인 광고에 포함된 지역 특색 도출하기\n",
    "1. 수집: RSS 피드로부터 수집. RSS 피드의 인터페이스 구축\n",
    "2. 준비: 토큰 벡터로 텍스트 구문 분석\n",
    "3. 분석: 구문 분석이 확실하게 되었는지 토큰을 검사\n",
    "4. 훈련: 이전에 생성한 trainNB0()를 사용\n",
    "5. 검사: 실질적으로 동작하는지 확인하기 위해 오류율을 확인. 오류율과 결과를 개선하기 위해 토큰화를 수정할 수 있음.\n",
    "6. 사용: 모든 상황을 함께 다루는 완전한 프로그램을 구축함. 두 가지 RSS 피드에서 얻은 가장 일반적인 단어를 표현함\n",
    "\n",
    "### 4.7.1 수집: RSS 피드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList,fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedFreq[:30]\n",
    "\n",
    "def localWords(feed1,feed0):\n",
    "    import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    top30Words = calcMostFreq(vocabList,fullText)\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = range(2*minLen); testSet=[]\n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != \\\n",
    "            classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print ('the error rate is: %s' %float(errorCount)/len(testSet))\n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2 분석: 지역적으로 사용되는 단어 표현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTopWords(ny, sf):\n",
    "    import operator\n",
    "    vocabList, p0V, p1V = localWords(ny, sf)\n",
    "    topNY=[]; topSF=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))\n",
    "        if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print (\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n",
    "    for item in sortedSF:\n",
    "        print (item[0])\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print (\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY **\")\n",
    "    for item in sortedNY:\n",
    "        print (item[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'localWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e225279a2d97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mny\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://newyork.craigslist.org/stp/index.rss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://sfbay.craigslist.org/stp/index.rss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgetTopWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mny\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-4875a33de502>\u001b[0m in \u001b[0;36mgetTopWords\u001b[1;34m(ny, sf)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetTopWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvocabList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp0V\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp1V\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocalWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtopNY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mtopSF\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp0V\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'localWords' is not defined"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "getTopWords(ny,sf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
